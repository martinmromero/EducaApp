# ðŸ“¦ Kit de IntegraciÃ³n Ollama - Listo para Exportar

**Esta carpeta es completamente independiente y lista para copiar a cualquier proyecto.**

âœ… Todo lo necesario para integrar IA (Ollama) en tus aplicaciones  
âœ… Ejemplos en Node.js, Python y HTML  
âœ… DocumentaciÃ³n completa  
âœ… Sin dependencias del proyecto original

---

## ðŸš€ Inicio RÃ¡pido (3 opciones)

### OpciÃ³n 1: Copiar toda la carpeta
```bash
# Copiar esta carpeta completa a tu proyecto
cp -r ejemplos-integracion /tu/nuevo/proyecto/ollama-integration
cd /tu/nuevo/proyecto/ollama-integration
```

### OpciÃ³n 2: Solo el cliente
```bash
# Copiar solo cliente-ollama-nodejs.js o cliente-ollama-python.py
npm install axios  # Para Node.js
# o
pip install requests  # Para Python
```

### OpciÃ³n 3: Backend completo
```bash
npm install
cp .env.example .env
node backend-express-minimo.js
```

---

## ðŸ“ Contenido

### 1. **cliente-ollama-nodejs.js**
Cliente completo en Node.js para interactuar con Ollama.

**Uso:**
```bash
npm install axios
node cliente-ollama-nodejs.js
```

**CaracterÃ­sticas:**
- âœ… GeneraciÃ³n de texto
- âœ… Chat con contexto
- âœ… Listar modelos
- âœ… Health check
- âœ… InformaciÃ³n de modelos

---

### 2. **cliente-ollama-python.py**
Cliente completo en Python para interactuar con Ollama.

**Uso:**
```bash
pip install requests
python cliente-ollama-python.py
```

**CaracterÃ­sticas:**
- âœ… GeneraciÃ³n de texto
- âœ… Chat con contexto
- âœ… Listar modelos
- âœ… Health check
- âœ… Type hints completos

---

### 3. **backend-express-minimo.js**
Backend Express completo con mÃºltiples endpoints para IA.

**InstalaciÃ³n:**
```bash
npm install express axios cors dotenv
```

**ConfiguraciÃ³n (.env):**
```env
PORT=3000
OLLAMA_HOST=localhost
OLLAMA_PORT=11434
OLLAMA_MODEL=llama3.1:8b
```

**Ejecutar:**
```bash
node backend-express-minimo.js
```

**Endpoints:**
- `GET /health` - Health check del backend
- `GET /api/ia/health` - Verificar conexiÃ³n con Ollama
- `GET /api/models` - Listar modelos disponibles
- `POST /api/chat` - Chat simple
- `POST /api/chat-context` - Chat con contexto
- `POST /api/generate` - GeneraciÃ³n avanzada con prompt personalizado

---

### 4. **frontend-html-minimo.html**
Interfaz web completa lista para usar.

**CaracterÃ­sticas:**
- âœ… Chat interfaz moderna
- âœ… SelecciÃ³n de modelo
- âœ… Indicador de estado
- âœ… Historial de conversaciÃ³n
- âœ… Metadata de respuestas (tokens, tiempo)
- âœ… DiseÃ±o responsive

**Uso:**
1. Abrir el archivo HTML en un navegador
2. Configurar URL del backend
3. Seleccionar modelo
4. Â¡Empezar a chatear!

---

## ðŸš€ Inicio RÃ¡pido

### OpciÃ³n 1: Todo Local

```bash
# 1. Instalar Ollama
curl https://ollama.ai/install.sh | sh

# 2. Descargar un modelo
ollama pull llama3.1:8b

# 3. Ejecutar backend
cd ejemplos-integracion
npm install express axios cors dotenv
node backend-express-minimo.js

# 4. Abrir frontend
# Hacer doble clic en frontend-html-minimo.html
```

### OpciÃ³n 2: Usar Cliente Directo (Sin Backend)

**JavaScript:**
```javascript
const OllamaClient = require('./cliente-ollama-nodejs');
const ollama = new OllamaClient('localhost', 11434);

const result = await ollama.generate('Hola, Â¿cÃ³mo estÃ¡s?');
console.log(result.text);
```

**Python:**
```python
from cliente_ollama_python import OllamaClient

ollama = OllamaClient('localhost', 11434)
result = ollama.generate('Hola, Â¿cÃ³mo estÃ¡s?')
print(result['text'])
```

---

## ðŸ“– DocumentaciÃ³n Completa

Para mÃ¡s informaciÃ³n, ver:
- **[GUIA_INTEGRACION_IA.md](../GUIA_INTEGRACION_IA.md)** - DocumentaciÃ³n completa
- **[Ollama API Docs](https://github.com/ollama/ollama/blob/main/docs/api.md)** - DocumentaciÃ³n oficial

---

## ðŸ”§ ConfiguraciÃ³n

### Variables de Entorno

Crear archivo `.env`:

```env
# Backend
PORT=3000

# Ollama
OLLAMA_HOST=localhost
OLLAMA_PORT=11434
OLLAMA_MODEL=llama3.1:8b

# ParÃ¡metros IA
AI_TEMPERATURE=0.7
AI_MAX_TOKENS=150
AI_TOP_P=0.9
```

### Modelos Recomendados

| Modelo | TamaÃ±o | Uso | RAM Necesaria |
|--------|--------|-----|---------------|
| `llama3.1:8b` | 4.7 GB | General | 8 GB |
| `gemma3:4b` | 2.5 GB | General (ligero) | 4 GB |
| `medgemma-4b-it-Q6_K` | 3.0 GB | Medicina | 6 GB |
| `mistral:7b` | 4.1 GB | General | 8 GB |

---

## ðŸ§ª Testing

### Probar ConexiÃ³n Ollama

```bash
curl http://localhost:11434/api/tags
```

### Probar Backend

```bash
# Health check
curl http://localhost:3000/health

# Chat simple
curl -X POST http://localhost:3000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Hola"}'
```

---

## ðŸ› Troubleshooting

### Error: ECONNREFUSED
**SoluciÃ³n:** Verificar que Ollama estÃ© corriendo
```bash
ollama serve
```

### Error: Model not found
**SoluciÃ³n:** Descargar el modelo
```bash
ollama pull llama3.1:8b
```

### Error: CORS
**SoluciÃ³n:** Agregar middleware CORS en backend
```javascript
app.use(cors());
```

---

## ðŸ“ Ejemplos de Peticiones

### Chat Simple
```javascript
fetch('http://localhost:3000/api/chat', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    message: 'Â¿QuÃ© es JavaScript?',
    model: 'llama3.1:8b',
    temperature: 0.7
  })
})
.then(res => res.json())
.then(data => console.log(data.reply));
```

### Chat con Contexto
```javascript
fetch('http://localhost:3000/api/chat-context', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    messages: [
      { role: 'user', content: 'Hola' },
      { role: 'assistant', content: 'Hola, Â¿en quÃ© puedo ayudarte?' },
      { role: 'user', content: 'Â¿De quÃ© hablamos antes?' }
    ]
  })
})
.then(res => res.json())
.then(data => console.log(data.reply));
```

---

## ðŸŽ¯ Casos de Uso

1. **Chatbot de atenciÃ³n al cliente**
2. **Asistente de programaciÃ³n**
3. **Tutor educativo**
4. **GeneraciÃ³n de contenido**
5. **AnÃ¡lisis de texto**
6. **Simulador de pacientes (como esta app)**

---

## ðŸ“š Recursos Adicionales

- [Ollama GitHub](https://github.com/ollama/ollama)
- [Ollama Models Library](https://ollama.ai/library)
- [Express.js Docs](https://expressjs.com/)
- [Axios Docs](https://axios-http.com/)

---

**Â¿Necesitas ayuda?** Todos estos ejemplos estÃ¡n probados y listos para usar. Solo ajusta las configuraciones a tu entorno.
